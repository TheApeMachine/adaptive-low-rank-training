\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{microtype}  % Better text flow, reduces overfull hbox
\usepackage[margin=1in]{geometry}  % Consistent margins

% Custom colors
\definecolor{accent}{RGB}{45, 106, 79}

\title{\textbf{Decoupled Bottleneck Attention:} \\[0.3em] 
\large Scaling Efficient Transformers via Low-Rank Semantic Routing}

\author{
  Daniel Owen van Dommelen\\
  \textit{Independent Research}\\
  \texttt{theapemachine@gmail.com}
}

\date{December 2025}

\begin{document}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
The Key-Value cache in Transformer models scales linearly with sequence length and model dimension, creating a critical memory bottleneck for long-context inference. While techniques like Grouped-Query Attention (GQA) reduce cache size by sharing key-value heads, they preserve the full computational cost of attention scoring in high-dimensional space.

We propose \textbf{Decoupled Bottleneck Attention}, an architectural modification that exploits the empirical observation that \textit{semantic routing}---deciding which tokens attend to which---operates in a low-rank subspace ($r \approx 32$), while \textit{positional geometry} requires higher fidelity ($r \approx 64$). By decoupling these concerns into separate projection paths, we achieve:
\begin{itemize}
    \item \textbf{Observed KV-cache savings at our experimental scale (v21/v29).} Reducing the attention interaction dimension yields a consistent $\approx 5.33\times$ FP16 KV-cache reduction in both our v21 toy suite ($d_{\text{model}}{=}512$, $d_{\text{attn}}{=}96$) and v29 scaling suite ($d_{\text{model}}{=}768$, $d_{\text{attn}}{=}144$); in our v29 implementation this corresponds to a measured $\sim$5.2$\times$ FP16 reduction at 128k context. Standard Q4\_0 KV-cache quantization composes with this to $\sim$19--21$\times$ end-to-end reduction at the same rank (Section~2.6).
    \item \textbf{Evidence at two scales:} v21 WikiText-2 ablations show that bottlenecking attention can outperform a full-rank baseline; v29 FineWeb-Edu scaling runs show that decoupling (48/96) outperforms a standard baseline, GQA (kv=2), and a matched-rank bottleneck baseline (Table~\ref{tab:fineweb})
    \item \textbf{Superior data efficiency compared to Grouped-Query Attention (GQA).} On FineWeb-Edu, our Decoupled Bottleneck model ($d_{\text{sem}}{=}48, d_{\text{geo}}{=}96$; total $d_{\text{attn}}=144$) achieves lower validation loss than a GQA baseline with shared KV heads (kv=2) under the same training recipe (mean over three seeds: $5.804 \pm 0.087$ vs $6.348 \pm 0.161$), demonstrating that reducing the \textit{interaction rank} is a more effective compression strategy than head sharing.
\end{itemize}

In our v21 WikiText-2 suite, a simple rank-96 bottleneck \textit{outperforms} the full rank-512 baseline (val loss 5.33 vs 5.37), suggesting that standard Transformers can over-allocate capacity to attention. In our v29 FineWeb-Edu suite, the Decoupled Bottleneck model achieves the best validation loss among baseline, GQA, and a matched-rank Bottleneck baseline (Table~\ref{tab:fineweb}).

\end{abstract}

\paragraph{Keywords:}
Transformer, attention mechanism, low-rank, KV-cache, memory efficiency, quantization, long context, rotary position embeddings

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}

Modern Transformer architectures \cite{vaswani2017attention} achieve remarkable performance across language modeling, translation, and reasoning tasks. However, their quadratic attention complexity and linear KV-cache growth present fundamental scalability challenges for long-context applications.

\subsection{The Redundancy Hypothesis}

We begin with a simple observation: in a 512-dimensional layer, the neurons are not independent. They move in \textit{sympathetic clusters}---correlated groups that reduce the intrinsic dimensionality of the representation. Prior work on LoRA \cite{hu2021lora} demonstrated that weight \textit{updates} during fine-tuning are low-rank (typically $r \leq 64$). Recent work on gradient dynamics \cite{refael2024adarankgrad} shows that optimization naturally collapses to low rank. We extend this observation to argue that the \textit{architecture itself}---specifically the attention mechanism---should be structurally constrained to match this intrinsic rank.

Empirical measurements from our experiments show that the \textit{Q/K projection activations} feeding attention have low entropy effective rank (typically in the tens of dimensions, far below the nominal width; Appendix~\ref{app:effective_rank}). This aligns with theoretical analysis by Bhojanapalli et al. \cite{bhojanapalli2020lowrank}, who identified a ``low-rank bottleneck'' in multi-head attention, and recent work by Kobayashi et al. \cite{kobayashi2024weightdecay} showing that weight decay actively induces rank reduction during training. Wang et al. \cite{wang2025lowdim} further demonstrate that attention outputs are approximately 60\% low-dimensional, adding to low-dimensional residual subspaces. Crucially, Refael et al. \cite{refael2024adarankgrad} proved that gradient rank \textit{decreases monotonically} during training, asymptotically approaching rank one---providing theoretical justification for why architectural bottlenecks become increasingly appropriate as training progresses. Chiang \& Yogatama \cite{chiang2025rope} show that RoPE may cause dimension inefficiency for long-distance retrieval, supporting our use of higher dimensions (64) for the geometric path.

\subsection{Comparison with Existing Approaches}

\paragraph{Grouped-Query Attention (GQA).}
While Grouped-Query Attention \cite{ainslie2023gqa} successfully reduces KV-cache memory by sharing key-value heads across multiple query heads, it maintains the full computational cost of the query projection and attention scoring in the high-dimensional space. Each query still operates in $\mathbb{R}^{d}$, and every attention score still requires a $d$-dimensional dot product---GQA merely amortizes the \textit{storage} cost, not the \textit{interaction} cost.

Our Bottleneck approach reduces both memory \textit{and} compute by compressing the interaction manifold. Rather than sharing high-dimensional KV pairs, we project queries and keys into a low-rank semantic subspace ($r \ll d$) \textit{before} computing attention, reducing dot-product complexity from $O(n^2 d)$ to $O(n^2 r)$.

\paragraph{Multi-Head Latent Attention (MLA).}
DeepSeek-V2 \cite{deepseek2024v2} introduced MLA, which compresses KV storage into a latent vector, achieving 93\% cache reduction. However, MLA \textit{up-projects} during the forward pass to perform attention in the original high-dimensional space. Our method remains low-rank throughout, saving both memory and compute.

\paragraph{Disentangled Attention.}
DeBERTa \cite{he2020deberta} pioneered the separation of content and position representations in attention scoring. We adopt this disentanglement principle but leverage it for \textit{efficiency}: applying aggressive compression to the semantic (content) path while preserving fidelity in the geometric (position) path.

\subsection{Contributions}

\begin{enumerate}
    \item We demonstrate that attention routing can be performed in $\sim$32 dimensions without perplexity degradation, while positional encoding requires $\sim$64 dimensions for RoPE fidelity.
    \item We propose \textbf{Decoupled Bottleneck Attention}, which separates semantic and geometric scoring paths with asymmetric dimensionality.
    \item We introduce and \textbf{evaluate} a \textbf{Null Token} mechanism that provides an explicit ``attend nowhere'' option (beneficial in some low-rank regimes, but not universally; Appendix~\ref{app:decoupled_ablations}).
    \item We report measured KV-cache footprints for our implementations and show that reducing the interaction dimension yields a consistent $\approx 5.33\times$ FP16 KV-cache reduction at our experimental scales (v21/v29). We discuss how this composes with standard KV-cache quantization; large fixed-rank scaling numbers are treated as non-validated arithmetic, not a central claim (Section~2.6).
\end{enumerate}

% ============================================================================
% RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related_work}

\paragraph{Low-rank and approximate attention.}
A long line of work seeks to reduce the quadratic cost of attention by approximating the score computation or constraining its rank. Linformer \cite{wang2020linformer} projects keys and values into a lower-dimensional subspace along the sequence dimension, yielding linear-time attention under a low-rank assumption. Kernel and hashing methods such as Performer \cite{choromanski2021performer} and Reformer \cite{kitaev2020reformer} similarly reduce attention cost via randomized features or locality-sensitive hashing. Our setting is different: we train standard causal LMs, but explicitly reduce the query/key interaction dimension inside each layer, targeting both compute (\(O(n^2 r)\)) and KV-cache memory (\(O(n r)\)).

\paragraph{Sparse/local attention for long documents.}
Sparse patterns (e.g., sliding window with global tokens) as in Longformer \cite{beltagy2020longformer} and BigBird \cite{zaheer2020bigbird} reduce attention compute while retaining access to distant context. However, for autoregressive decoding these methods still accumulate a KV cache whose size grows linearly with context length. Our work instead reduces the per-token cache footprint, which is complementary to sparse attention and other long-context strategies \cite{huang2023longcontextsurvey}.

\paragraph{KV-cache optimization.}
Sharing KV heads reduces cache storage by amortizing keys and values across query heads (MQA/GQA) \cite{shazeer2019mqa,ainslie2023gqa}. Latent KV schemes such as MLA compress the cache into a lower-dimensional latent that is expanded during attention \cite{deepseek2024v2}. Orthogonally, quantizing the KV cache reduces memory at fixed architecture \cite{hooper2024kvquant,li2025commvq}. Our decoupled bottleneck reduces the interaction dimension before scoring (saving compute) and also makes heterogeneous KV quantization natural: semantic keys can often be quantized more aggressively than geometric keys.

\paragraph{Expressiveness limits and structured alternatives.}
Reducing interaction rank too far can harm representation power: theory and empirical evidence show regimes where increasing heads under fixed head dimension does not recover lost capacity \cite{bhojanapalli2020lowrank,amsel2025qualityheads}. Recent structured-matrix formulations aim to increase effective rank without full cost by parameterizing attention maps with richer structured operators \cite{kuang2025structuredmatrices}. Decoupling is a simple architectural compromise: we keep a higher-dimensional geometric path (with RoPE) while aggressively compressing only the semantic routing path.

% ============================================================================
% 2. METHODOLOGY
% ============================================================================
\section{Methodology}

\subsection{Standard Multi-Head Attention}

In standard scaled dot-product attention with $H$ heads:
\begin{equation}
    \text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\end{equation}
where $Q, K, V \in \mathbb{R}^{n \times d}$ are obtained by linear projection from the input $X \in \mathbb{R}^{n \times d_{\text{model}}}$:
\begin{equation}
    Q = XW_Q, \quad K = XW_K, \quad V = XW_V
\end{equation}
with $W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d}$. For language modeling with context length $n$ and dimension $d$, the KV-cache requires $O(2 \cdot L \cdot n \cdot d)$ memory, where $L$ is the number of layers.

\subsection{Bottleneck Attention}

We introduce a simple modification: project $Q$ and $K$ to a lower-dimensional space \textit{before} computing attention scores.\footnote{Our use of ``bottleneck'' refers to dimensionality reduction in the query/key space, distinct from Park et al.'s BAM \cite{park2018bam}, which applies channel and spatial attention in CNNs for computer vision.}
\begin{equation}
    Q' = XW_Q', \quad K' = XW_K'
\end{equation}
where $W_Q', W_K' \in \mathbb{R}^{d_{\text{model}} \times d_{\text{attn}}}$ with $d_{\text{attn}} \ll d_{\text{model}}$. The attention computation becomes:
\begin{equation}
    \text{Attn}_{\text{bottleneck}}(Q', K', V') = \text{softmax}\left(\frac{Q'K'^\top}{\sqrt{d_{\text{attn}}/H}}\right) V'
\end{equation}

This reduces the dot-product complexity from $O(n^2 \cdot d_{\text{model}})$ to $O(n^2 \cdot d_{\text{attn}})$ and the KV-cache from $O(n \cdot d_{\text{model}})$ to $O(n \cdot d_{\text{attn}})$.

\subsection{Decoupled Bottleneck Attention}

The key insight motivating decoupling is that \textit{semantic matching} (``is this token semantically related?'') and \textit{geometric positioning} (``how far away is this token?'') have different intrinsic dimensionality requirements.

We decompose the attention score into two additive components:
\begin{equation}
    \text{Score} = \underbrace{\frac{Q_{\text{sem}} K_{\text{sem}}^\top}{\sqrt{d_{\text{sem}}/H}}}_{\text{Semantic Path}} + \underbrace{\frac{Q_{\text{geo}} K_{\text{geo}}^\top}{\sqrt{d_{\text{geo}}/H}}}_{\text{Geometric Path}}
\end{equation}

where:
\begin{align}
    Q_{\text{sem}} &= XW_{Q,\text{sem}}, \quad K_{\text{sem}} = XW_{K,\text{sem}} \quad &(d_{\text{sem}} = 32) \\
    Q_{\text{geo}} &= XW_{Q,\text{geo}}, \quad K_{\text{geo}} = XW_{K,\text{geo}} \quad &(d_{\text{geo}} = 64)
\end{align}

Critically, we apply \textbf{Rotary Position Embeddings (RoPE)} \cite{su2021roformer} \textit{only} to the geometric path:
\begin{equation}
    Q_{\text{geo}}, K_{\text{geo}} \leftarrow \text{RoPE}(Q_{\text{geo}}, K_{\text{geo}}, \text{position})
\end{equation}

The semantic path operates on pure content similarity, while the geometric path encodes positional relationships. The value projection uses the combined dimension:
\begin{equation}
    V = XW_V, \quad W_V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{attn}}}
\end{equation}
where $d_{\text{attn}} = d_{\text{sem}} + d_{\text{geo}}$. In our smaller v21 suite we use a 32/64 split ($d_{\text{attn}}{=}96$), and in our v29 scaling suite we use a 48/96 split ($d_{\text{attn}}{=}144$).

\subsection{The Null Token Mechanism}

Low-rank attention can become unstable when queries lack semantically appropriate keys. We introduce a learnable \textbf{null token} $k_\emptyset$ providing an explicit ``attend nowhere'' option:
\begin{equation}
    \text{Score}_{\text{null}} = \frac{Q_{\text{sem}} k_{\emptyset,\text{sem}}^\top}{\sqrt{d_{\text{sem}}/H}} + \frac{Q_{\text{geo}} k_{\emptyset,\text{geo}}^\top}{\sqrt{d_{\text{geo}}/H}}
\end{equation}

This score is concatenated to the attention matrix before softmax, allowing the model to ``dump'' attention mass when no key is appropriate, which can stabilize training at very low ranks. In our v29 48/96 setting, however, the null token is not beneficial under this recipe (Appendix~\ref{app:decoupled_ablations}).

\subsection{Tied Q-K Projections}

For the semantic path, we optionally \textbf{tie} the query and key projections: $W_{Q,\text{sem}} = W_{K,\text{sem}}$. This enforces symmetric similarity (``A attends to B iff B attends to A''), which is appropriate for content matching but not for position-dependent relationships.

\subsection{Quantized Inference}

For inference, we apply aggressive quantization to the KV-cache. Recent work has demonstrated that 4-bit KV cache quantization preserves model quality remarkably well. Turboderp's ExLlamaV2 implementation \cite{turboderp2024qcache} showed Q4 cache performs comparably to FP16, and this capability has been integrated into production inference engines like llama.cpp \cite{llamacpp2024kvcache}. We implement block-wise Q4\_0 quantization following this approach:
\begin{equation}
    x_{\text{quantized}} = \text{round}\left(\frac{x}{\text{scale}}\right), \quad \text{scale} = \frac{\max(|x_{\text{block}}|)}{7}
\end{equation}
where each block of 32 elements shares a single FP16 scale factor. In the idealized limit (ignoring scale metadata) 4-bit values correspond to 0.5 bytes/value. With Q4\_0 block scales, the effective bytes/value is slightly larger (18 bytes per 32 values $\Rightarrow$ 0.5625 bytes/value), so the ideal 4$\times$ factor becomes $\approx 3.56\times$ in practice. Combined with dimension reduction, the per-layer KV-cache reduction is approximately:
\begin{equation}
    \text{Compression} \approx \underbrace{\frac{d_{\text{model}}}{d_{\text{attn}}}}_{\text{Dimension}} \times \underbrace{\frac{2~\text{bytes}}{0.5~\text{bytes}}}_{\text{Ideal Q4}} \;\;\approx\;\; \frac{d_{\text{model}}}{d_{\text{attn}}}\times 4.
\end{equation}
Both our v21 (512$\rightarrow$96) and v29 (768$\rightarrow$144) settings yield the same dimension factor (5.33$\times$), implying $\sim$19--21$\times$ end-to-end FP16$\rightarrow$Q4 savings depending on quantization overheads.

\paragraph{Scaling arithmetic (context only; not validated at scale).}
The KV-cache memory at long context depends on the choice of attention dimension $d_{\text{attn}}$ at scale. For a rough Llama-like configuration (32 layers, $d_{\text{model}}=4096$, 128k context, batch=1), the FP16 KV cache is:
\[
M_{\text{FP16}} \approx 2 \cdot 32 \cdot 4096 \cdot 128\text{k} \cdot 2 \text{ bytes} \approx 64~\text{GiB}.
\]
With 4-bit KV-cache quantization (idealized 0.5 bytes/value), the memory becomes:
\[
M_{\text{Q4}} \approx 2 \cdot 32 \cdot d_{\text{attn}} \cdot 128\text{k} \cdot 0.5 \text{ bytes}.
\]
This yields two reference scenarios (for context):
\begin{itemize}
    \item \textbf{Constant-fraction $d_{\text{attn}}$ (e.g., $d_{\text{attn}}=768$):} $M_{\text{Q4}} \approx 3.0$~GiB, for an overall reduction of $\sim 21\times$.
    \item \textbf{Speculative fixed-rank $d_{\text{attn}}$ (intuition only):} If one could keep $d_{\text{attn}}$ roughly constant while scaling $d_{\text{model}}$ (e.g., $d_{\text{attn}}{=}96$ at $d_{\text{model}}{=}4096$), the same linear arithmetic yields an $\mathcal{O}(10^2)$ reduction versus a standard FP16 baseline.\footnote{This is the origin of the often-quoted ``168$\times$'': \((4096/96)\times 4 \approx 171\), sometimes rounded. Including Q4\_0 scale metadata gives \((4096/96)\times (2/0.5625)\approx 152\). We do \emph{not} validate fixed-rank scaling in this work.}
\end{itemize}
The architectural contribution is the \textit{dimension reduction} (the ratio $4096/d_{\text{attn}}$); the additional factor of $4\times$ comes from standard 16$\rightarrow$4-bit quantization (idealized). For fair comparisons, note that GQA caches can also be quantized; e.g., an $8\times$ GQA KV cache with Q4 would already yield $\sim$32$\times$ reduction vs FP16 standard. We therefore treat fixed-rank scaling numbers as back-of-the-envelope upper bounds, not a primary experimental claim.

\paragraph{Heterogeneous KV-cache quantization (decoupled).}
A practical benefit of decoupling is that it enables \textit{heterogeneous} KV-cache quantization: we can compress the semantic path more aggressively (e.g., Q4) while keeping the geometric (RoPE) path at higher fidelity (e.g., Q8). As a lightweight sanity check, we compared FP16 caches to a decoupled policy with $K_{\text{sem}}=\text{Q4}$, $K_{\text{geo}}=\text{Q8}$, $V=\text{Q4}$ and a 128-token FP16 residual window. On a small held-out calibration slice from FineWeb-Edu, this policy yielded $\Delta$NLL $\approx 0.015$ nats/token (perplexity ratio $\approx 1.015$) and $\mathrm{KL}(p_{\text{FP16}} \| p_{\text{quant}}) \approx 0.006$ nats/token, while preserving identical greedy decoding for short prompts. These guardrail metrics suggest that aggressive semantic compression can be applied without catastrophic degradation, even though stochastic sampling trajectories may diverge due to small distribution shifts.

While we report training throughput in our experiments, the theoretical FLOPs reduction in the attention mechanism ($O(n^2 d) \to O(n^2 r)$) implies a proportional speedup in the \textit{prefill phase} of inference, where the KV-cache is populated. For autoregressive decoding, the memory bandwidth savings from the smaller cache dominate latency improvements.

% ============================================================================
% 3. EXPERIMENTS
% ============================================================================
\section{Experiments}

\subsection{Experimental Setup}

\paragraph{Why two suites?}
We use a two-stage experimental strategy: the \textbf{v21 ablation suite} (WikiText-2, smaller model) enables rapid iteration and mechanistic insight, while the \textbf{v29 scaling suite} (FineWeb-Edu, larger model) tests whether the same architectural ideas hold at higher scale. Because these suites differ in model size, data distribution, and training details, we report them separately and avoid direct numerical comparison across suites.

\paragraph{Model Configuration.}
We report results from two experiment suites:
\begin{itemize}
    \item \textbf{v21 (ablation suite):} a smaller 6-layer model ($d_{\text{model}}{=}512$) trained on WikiText-2 for rapid architectural iteration.
    \item \textbf{v29 (scaling suite):} a larger 12-layer model ($d_{\text{model}}{=}768$, 12 heads, $d_{\text{ff}}{=}3072$) trained on FineWeb-Edu (100M-token dataset) to validate scaling behavior.
\end{itemize}

\paragraph{Datasets.}
\begin{itemize}
    \item \textbf{WikiText-2}: 2M tokens of Wikipedia text used for rapid prototyping in the v21 ablation suite.
    \item \textbf{FineWeb-Edu}: a 100M-token dataset of educational web content used for the v29 scaling suite. In our implementation, we use a 50,257-token vocabulary (GPT-2 compatible) for tokenized data.
\end{itemize}

\paragraph{Training.}
Unless otherwise noted, v29 models are trained for 6000 steps with:
\begin{itemize}
    \item Lion optimizer ($\beta_1{=}0.9,\beta_2{=}0.99$), constant learning rate $3\times 10^{-4}$, weight decay 0.1, warmup 0
    \item Batch size 8 with gradient accumulation 2 (global batch = 16 sequences/update), gradient clipping at 1.0
    \item Training context length: 256 tokens (block size 256); RoPE extrapolation is evaluated separately at 2$\times$ and 4$\times$ this length (Appendix~\ref{app:long_context})
    \item BF16 parameters with AMP (BF16), dropout 0.0, SwiGLU MLP
\end{itemize}
All v29 runs in Table~\ref{tab:fineweb} were executed with the same recipe on Apple Silicon via MPS; full configurations are recorded in the run logs.

\subsection{v21 Ablation Suite: WikiText-2 Results}

\begin{table}[htbp]
\centering
\small
\caption{WikiText-2 Validation Loss Comparison (v21 suite; 6-layer model)}
\label{tab:wikitext_v21}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Attn Config} & \textbf{Params} & \textbf{Val Loss} & \textbf{Tok/s} \\
\midrule
Standard Baseline & $d = 512$ & 31.8M & 5.37 & 20k \\
\textbf{Combined 96} & $d_{\text{attn}} = 96$ & 30.1M & \textbf{5.33} & 117k \\
Bottleneck 128 & $d_{\text{attn}} = 128$ & 31.3M & 5.48 & 128k \\
Decoupled 32/64 & $d_{\text{sem}}{=}32, d_{\text{geo}}{=}64$ & 30.9M & 5.59 & 106k \\
GQA (kv=2) & 8Q/2KV heads & 30.1M & 5.63 & 25k \\
Small Model & $d_{\text{model}} = 128$ & 4.2M & 5.74 & 930k \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Finding (v21).}
Table~\ref{tab:wikitext_v21} shows that the Combined 96 bottleneck achieves the \textit{lowest} validation loss (5.33), outperforming the full-rank baseline (5.37). This supports the hypothesis that attention routing can be learned in a substantially lower-dimensional interaction space.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/convergence_wikitext.png}}
\caption{Validation loss curves on WikiText-2 (v21 suite). Bottlenecked models converge rapidly; Combined 96 achieves the best final loss.}
\label{fig:convergence_wikitext}
\end{figure}

\subsection{v29 Scaling Suite: FineWeb-Edu Results}

To validate that our findings generalize beyond small datasets, we train on a 100M-token FineWeb-Edu dataset (Figure~\ref{fig:convergence_fineweb}).

\begin{table}[htbp]
\centering
\small
\caption{FineWeb-Edu Validation Loss (100M-token dataset; 256 context; 6000 steps; v29 suite; mean $\pm$ std over three seeds)}
\label{tab:fineweb}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Attn Config} & \textbf{Params} & \textbf{Val Loss} & \textbf{Val PPL} \\
\midrule
Standard Baseline & $d_{\text{attn}}=d_{\text{model}}=768$ & 139.8M & $6.385 \pm 0.222$ & $602.9 \pm 140.4$ \\
GQA (kv=2) & 12Q/2KV & 128.0M & $6.348 \pm 0.161$ & $576.5 \pm 95.5$ \\
Bottleneck (rank 144) & $d_{\text{attn}}=144$ & 116.8M & $6.424 \pm 0.090$ & $617.9 \pm 56.9$ \\
\textbf{Decoupled 48/96} & $d_{\text{sem}}{=}48, d_{\text{geo}}{=}96$ & \textbf{116.8M} & $\mathbf{5.804 \pm 0.087}$ & $\mathbf{332.3 \pm 28.5}$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Scaling Observation.}
On FineWeb-Edu (100M-token dataset), the Decoupled Bottleneck model \textit{outperformed} the Standard Baseline, a reduced-KV GQA baseline (kv=2), and a matched-rank Bottleneck baseline (Table~\ref{tab:fineweb}). We observe substantial run-to-run variability for the full-rank baseline under this short training regime; therefore, we report mean $\pm$ std over three seeds in Table~\ref{tab:fineweb}, and show representative learning curves for a single seed in Figure~\ref{fig:convergence_fineweb}.
To address baseline tuning concerns, we include a small baseline learning-rate sweep and a longer-horizon comparison (seed 1337) in Appendix~\ref{app:baseline_fairness}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1337_convergence.png}}
\caption{Validation loss curves on FineWeb-Edu (100M-token dataset; seed 1337). Under the same training setup, the Decoupled Bottleneck model converges to the lowest validation loss, outperforming both the standard baseline and GQA.}
\label{fig:convergence_fineweb}
\end{figure}

\subsection{Final Loss Comparison}

Figure~\ref{fig:comparison_bar} provides a direct comparison of final validation losses across our FineWeb-Edu (v29) architectures. The Decoupled Bottleneck (48/96) achieves the best validation loss among baseline, GQA, and bottleneck variants.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1337_comparison_bar.png}}
\caption{Final validation loss comparison (FineWeb-Edu, v29; seed 1337). Lower is better. Decoupled 48/96 achieves the best loss, outperforming both the standard baseline and a GQA baseline (kv=2).}
\label{fig:comparison_bar}
\end{figure}

\subsection{Ablation Studies}

\paragraph{Wide Residual Stream Hypothesis.}
Comparing ``Small Model'' ($d_{\text{model}} = 128$) to ``Bottleneck 128'' ($d_{\text{model}} = 512$, $d_{\text{attn}} = 128$), we observe a 0.26 loss gap (5.74 vs 5.48) and severe overfitting in the small model. This confirms that the \textit{residual stream} must remain wide; only the \textit{attention interaction} can be compressed.

\paragraph{GQA vs. Bottleneck (Head Sharing vs. Interaction Rank).}
We compared our method against Grouped-Query Attention (GQA) with matched KV memory footprints on FineWeb-Edu. While GQA reduces KV cache storage by sharing key-value heads, it retains full-rank query projections and attention scoring in high-dimensional space. In contrast, the Bottleneck architecture reduces the interaction dimension directly. In our v29 runs, both GQA (kv=2) and Bottleneck (rank 144) reduce KV-cache memory to \(\sim 0.75\)~GB and \(\sim 0.84\)~GB at 128k context respectively, but Bottleneck did not consistently improve validation loss over GQA under this recipe (Table~\ref{tab:fineweb}).

\paragraph{Decoupled vs. Bottleneck (Separating Content and Geometry).}
Holding the same total attention dimension fixed ($d_{\text{attn}}=144$), the Decoupled Bottleneck model (48/96) improves validation loss at essentially the same KV cache footprint (\(\sim 0.84\)~GB at 128k context), suggesting that semantic/geometric separation can improve optimization and generalization at this scale (Table~\ref{tab:fineweb}).

\paragraph{Long Context Stability.}
Appendix~\ref{app:long_context} reports two lightweight probes: teacher-forced RoPE extrapolation (256/512/1024 contexts) and a passkey needle-in-a-haystack prompt. These probes are intended as regression tests (not downstream evaluations); we report likelihood-based needle effects because strict next-token retrieval accuracy is often 0\% for base LMs.

\subsection{Memory-Quality Trade-off}

Figure~\ref{fig:pareto} shows the quality--efficiency trade-off for our FineWeb-Edu (v29) runs. For these experiments, KV-cache memory at 128k context is measured directly from the implementation, and we compare it against the best validation loss achieved by each architecture. Notably, Decoupled 48/96 matches the Bottleneck's KV footprint (same total $d_{\text{attn}}$) while improving quality, and it outperforms GQA despite GQA's smaller KV cache.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1337_pareto.png}}
\caption{Quality--efficiency comparison on FineWeb-Edu (v29). Points show best validation loss versus attention dimension ($d_{\text{attn}}$), with annotations derived from measured KV-cache memory at 128k context.}
\label{fig:pareto}
\end{figure}

\subsection{Memory Footprint Analysis}

Table~\ref{tab:memory} gives an illustrative KV-cache scaling projection for a 128k context in a Llama-like configuration (32 layers, $d_{\text{model}} = 4096$). We intentionally \emph{do not} foreground optimistic fixed-rank ``upper bound'' numbers here; the experimentally grounded takeaway is the linear dependence on $d_{\text{attn}}$ and the $\approx 5.33\times$ FP16 reduction that follows from our toy-scale choices ($512\!\rightarrow\!96$ in v21; $768\!\rightarrow\!144$ in v29), which we measure directly in our v29 implementation.

\begin{table}[htbp]
\centering
\small
\caption{KV-Cache Memory for 128k Context (Llama-like scale; projected)}
\label{tab:memory}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Architecture} & \textbf{VRAM} & \textbf{Compression} \\
\midrule
Standard (FP16) & 64.0 GB & 1$\times$ \\
GQA (32Q/4KV; FP16) & 8.0 GB & 8$\times$ \\
GQA (32Q/4KV; Q4, ideal) & 2.0 GB & 32$\times$ \\
MLA (FP16) & 4.3 GB & 15$\times$ \\
Bottleneck (FP16) & 1.5 GB & 43$\times$ \\
Decoupled (Q4, constant-fraction $d_{\text{attn}}{=}768$) & 3.0 GB & 21$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{\detokenize{assets/memory_footprint.png}}
\caption{KV-cache memory comparison at 128k context (illustrative scaling projection). The experimentally grounded point is that KV-cache memory scales linearly with $d_{\text{attn}}$; combining this with standard KV-cache quantization yields multiplicative savings. Large fixed-rank upper-bound numbers are omitted from the main narrative and treated as speculative arithmetic (Section~2.6).}
\label{fig:memory}
\end{figure}

% ============================================================================
% 4. DISCUSSION
% ============================================================================
\section{Discussion}

\subsection{Why Does Low-Rank Attention Work?}

We hypothesize two complementary explanations:

\paragraph{Intrinsic Dimensionality.}
Following Aghajanyan et al. \cite{aghajanyan2021intrinsic}, natural language representations lie on low-dimensional manifolds. The attention mechanism's role is \textit{routing}---selecting which tokens to aggregate---not computing complex transformations. Routing decisions are inherently low-entropy and thus low-rank.

\paragraph{Regularization Effect.}
The bottleneck acts as an implicit regularizer, preventing the model from memorizing spurious token-pair correlations. In our v29 FineWeb runs, reducing the interaction rank improves generalization relative to the full-rank baseline.

\paragraph{Gradient Rank Dynamics.}
AdaRankGrad \cite{refael2024adarankgrad} proves that gradient rank decreases monotonically during training, eventually approaching rank one. This suggests that \textit{architectural} bottlenecks become increasingly appropriate as training progresses---the model naturally ``wants'' to operate in a low-rank subspace. By hard-wiring this constraint from the start, we may accelerate convergence by matching the architecture to the optimization landscape.

\subsection{When to Use Each Architecture}

Our experiments reveal a two-stage story: v21 ablations on WikiText-2 are useful for rapid iteration and mechanistic insight, while v29 FineWeb-Edu results validate the key claims at larger scale.

\begin{itemize}
    \item \textbf{Decoupled Bottleneck:} On FineWeb-Edu (v29), Decoupled 48/96 achieves the best validation loss among the tested variants (baseline, GQA, and bottleneck) while preserving the KV memory benefits of low-rank attention. It is also the \textit{only} architecture enabling \textbf{heterogeneous quantization}---e.g., Q4 for semantic and Q8 for geometric paths---which is critical for long-context inference deployments.
    \item \textbf{Standard Attention:} A strong baseline and simplest implementation, but can be memory-inefficient for long contexts.
\end{itemize}

\paragraph{Recommendation.}
For \textit{training}, use the v21 ablation suite to quickly explore attention-rank and decoupling choices, then validate at scale with the v29 FineWeb suite. For \textit{inference} under memory constraints, convert to Decoupled with heterogeneous quantization (aggressively compress semantic, preserve geometric fidelity).

\paragraph{Flash/SDPA compatibility.}
Decoupled Bottleneck Attention can be implemented using PyTorch's fused \texttt{scaled\_dot\_product\_attention} by concatenating the scaled semantic and geometric Q/K projections along the head dimension, making it compatible with modern Flash Attention kernels.

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Sensitivity to setup:} Results can depend on the training recipe and implementation details. Earlier (v21) FineWeb runs showed a degradation for decoupled attention, whereas the v29 decoupled model improves substantially. This motivates replication across seeds, longer training, and additional datasets.
    \item Experiments are limited to $\sim$140M parameter models (up to $d_{\text{model}}{=}768$) trained on a 100M-token dataset. Verification at 7B+ scale is needed.
    \item The optimal $(d_{\text{sem}}, d_{\text{geo}})$ split may vary with model scale.
    \item We have not evaluated on downstream tasks (e.g., MMLU, HellaSwag).
    \item Throughput measurements are from training; inference latency benchmarks are future work.
\end{itemize}

% ============================================================================
% 5. CONCLUSION
% ============================================================================
\section{Conclusion}

We have demonstrated that attention in Transformers contains significant redundancy. On FineWeb-Edu (100M-token dataset), the Decoupled Bottleneck architecture \textbf{surpassed} the standard baseline, a GQA baseline (kv=2), and a matched-rank Bottleneck baseline under the same training recipe (Table~\ref{tab:fineweb}). These results suggest that, at this scale and training horizon, standard high-dimensional attention can be over-parameterized; larger-scale validation and longer training are important future work.

The core insight is architectural: \textbf{Attention is a router, not a processor.} The heavy computation should happen in the feedforward layers (which we leave at full rank), while attention merely selects which tokens to aggregate. By matching the architecture to this functional role, we unlock dramatic efficiency gains.

Our Decoupled Bottleneck Attention separates semantic matching from positional geometry, allowing aggressive compression on the former while preserving RoPE fidelity on the latter. Combined with 4-bit KV-cache quantization, the memory arithmetic suggests that 128k-context inference can become \textit{feasible} on consumer hardware under fixed-rank scaling assumptions (Figure~\ref{fig:memory}); however, this is a projection and we do not claim validated 128k \textit{quality} in this work.

\paragraph{Future Work.}
We plan to: (1) validate at 7B+ scale where the efficiency gains compound; (2) explore learned mixing weights between semantic and geometric paths; (3) test robustness across seeds, longer training, and additional datasets to understand when decoupling improves optimization; and (4) benchmark inference latency on production hardware. In parallel, we are migrating from the monolithic research prototype to a modular production-oriented implementation (see the repository's \path{production/} code), with an evolving self-optimization layer for KV-cache policies and resource-aware inference; future work will validate functional parity and quantify real-world latency/memory trade-offs.

% ============================================================================
% STATEMENTS
% ============================================================================
\section*{Statements and Declarations}

\paragraph{Conflict of Interest.}
The author declares no competing interests. This research was conducted independently without corporate affiliation or funding from entities with financial interests in the outcomes.

\paragraph{Data Availability.}
All datasets used in this study are publicly available: WikiText-2 \cite{merity2016pointer} is available from Salesforce Research, and FineWeb-Edu is available from Hugging Face. The code, trained model checkpoints, and all experimental logs are available at \url{https://github.com/theapemachine/experiments}. For convenience and long-term artifact access, we also provide a mirror containing the original run directories (including all checkpoints) at \url{https://drive.google.com/drive/folders/1sQYMijP06a4Mf_1sEj_gLkE8OOEfUCB6?usp=drive_link}.

\paragraph{Funding.}
This research was conducted without external funding. All computational resources were provided by the author.

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Replication Plots for FineWeb-Edu (v29; Seeds 1338 and 1339)}
\label{app:v29_seed1338}

Table~\ref{tab:fineweb} reports mean $\pm$ std over three seeds. In the main text, we show representative learning curves and summary plots for seed 1337; here we include the corresponding plots for seeds 1338 and 1339 to make the replication evidence explicit.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1338_convergence.png}}
\caption{Validation loss curves on FineWeb-Edu (100M-token dataset; seed 1338).}
\label{fig:convergence_fineweb_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1338_early_convergence.png}}
\caption{Early convergence view on FineWeb-Edu (seed 1338), highlighting the first phase of optimization.}
\label{fig:early_convergence_fineweb_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1338_comparison_bar.png}}
\caption{Final validation loss comparison on FineWeb-Edu (v29; seed 1338). Lower is better.}
\label{fig:comparison_bar_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1338_pareto.png}}
\caption{Quality--efficiency comparison on FineWeb-Edu (v29; seed 1338).}
\label{fig:pareto_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{\detokenize{assets/m4max_seed1338_kv_memory_128k.png}}
\caption{Measured KV-cache memory at 128k context (v29; seed 1338).}
\label{fig:kv_memory_128k_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1339_convergence.png}}
\caption{Validation loss curves on FineWeb-Edu (100M-token dataset; seed 1339).}
\label{fig:convergence_fineweb_seed1339}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1339_early_convergence.png}}
\caption{Early convergence view on FineWeb-Edu (seed 1339), highlighting the first phase of optimization.}
\label{fig:early_convergence_fineweb_seed1339}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1339_comparison_bar.png}}
\caption{Final validation loss comparison on FineWeb-Edu (v29; seed 1339). Lower is better.}
\label{fig:comparison_bar_seed1339}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1339_pareto.png}}
\caption{Quality--efficiency comparison on FineWeb-Edu (v29; seed 1339).}
\label{fig:pareto_seed1339}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{\detokenize{assets/m4max_seed1339_kv_memory_128k.png}}
\caption{Measured KV-cache memory at 128k context (v29; seed 1339).}
\label{fig:kv_memory_128k_seed1339}
\end{figure}

\section{Effective Rank Evidence (Q/K Projection Activations)}
\label{app:effective_rank}

We compute singular value spectra and entropy-based effective rank for the \textit{Q/K projection activations} (post-linear outputs feeding attention) in trained checkpoints, by capturing projection outputs on a fixed token slice and computing singular values of their empirical covariance. We use the standard entropy effective rank:
\[
\mathrm{eRank} = \exp(H(p)), \quad p_i=\frac{\sigma_i}{\sum_j \sigma_j},
\]
where $\{\sigma_i\}$ are singular values and $H(\cdot)$ is Shannon entropy. The script to reproduce this figure is included in the repository; the figure below is generated automatically when that script is run.

\begin{figure}[htbp]
\centering
\IfFileExists{assets/m4max_rank_evidence.png}{
  \includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_rank_evidence.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \path{assets/m4max_rank_evidence.png}.\\
  Run the rank analysis script to generate this figure.}}
}
\caption{Singular value spectra and entropy-based effective rank for Q/K projection activations across layers (example: v29 checkpoints).}
\label{fig:rank_evidence}
\end{figure}

\section{Long-Context Sanity Checks}
\label{app:long_context}

To complement the theoretical KV-cache scaling analysis, we include two lightweight long-context probes: (1) teacher-forced loss at increasing context lengths (RoPE extrapolation), and (2) a passkey ``needle-in-a-haystack'' next-token retrieval probe. These are not full downstream evaluations, but they help catch obvious long-context regressions and RoPE failures.

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_baseline_rope_extrapolation.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_baseline_rope_extrapolation.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_baseline_rope_extrapolation.png}}}
}
\caption{Baseline}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_decoupled_rope_extrapolation.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_decoupled_rope_extrapolation.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_decoupled_rope_extrapolation.png}}}
}
\caption{Decoupled}
\end{subfigure}
\caption{RoPE extrapolation probe (teacher-forced). Figures are generated by \texttt{test\_rope\_extrapolation\_v29.py}.}
\label{fig:rope_extrapolation_appendix}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_baseline_needle_haystack.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_baseline_needle_haystack.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_baseline_needle_haystack.png}}}
}
\caption{Baseline}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_decoupled_needle_haystack.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_decoupled_needle_haystack.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_decoupled_needle_haystack.png}}}
}
\caption{Decoupled}
\end{subfigure}
\vspace{0.75em}

\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_baseline_needle_haystack_delta_nll.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_baseline_needle_haystack_delta_nll.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_baseline_needle_haystack_delta_nll.png}}}
}
\caption{Baseline: $\Delta$NLL (with needle $-$ no needle)}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_decoupled_needle_haystack_delta_nll.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_decoupled_needle_haystack_delta_nll.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_decoupled_needle_haystack_delta_nll.png}}}
}
\caption{Decoupled: $\Delta$NLL (with needle $-$ no needle)}
\end{subfigure}

\vspace{0.75em}

\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_baseline_needle_haystack_log10_p_ratio.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_baseline_needle_haystack_log10_p_ratio.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_baseline_needle_haystack_log10_p_ratio.png}}}
}
\caption{Baseline: $\log_{10}\!\left(p_{\text{with}}/p_{\text{without}}\right)$}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_decoupled_needle_haystack_log10_p_ratio.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_decoupled_needle_haystack_log10_p_ratio.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_decoupled_needle_haystack_log10_p_ratio.png}}}
}
\caption{Decoupled: $\log_{10}\!\left(p_{\text{with}}/p_{\text{without}}\right)$}
\end{subfigure}

\caption{Passkey needle-in-a-haystack probe. Top row: strict next-token accuracy (often all-zero for base LMs). Middle/bottom: likelihood-based needle effect metrics which remain informative even when strict accuracy is zero. Figures are generated by \texttt{test\_needle\_haystack\_v29.py}.}
\label{fig:needle_haystack_appendix}
\end{figure}

\section{Decoupled Ablations (v29; Seed 1337)}
\label{app:decoupled_ablations}

To support claims about optional stabilizers (null token, tied Q--K projections) and positional encoding, we report a small ablation suite for Decoupled 48/96 on FineWeb-Edu (6000 steps; seed 1337). Lower is better.

\begin{table}[htbp]
\centering
\small
\caption{Decoupled 48/96 ablations (FineWeb-Edu v29; seed 1337; 6000 steps).}
\label{tab:decoupled_ablations}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Variant} & \textbf{Best Val Loss} & \textbf{$\Delta$ vs base} \\
\midrule
Decoupled 48/96 (base) & 5.879 & +0.000 \\
$-$ null token (\texttt{no\_null}) & 5.843 & $-0.035$ \\
$+$ tied Q--K (\texttt{tie\_qk}) & 5.926 & $+0.047$ \\
$+$ null token (\texttt{null}) & 6.395 & $+0.516$ \\
$-$ RoPE (\texttt{no\_rope}) & 6.446 & $+0.567$ \\
\bottomrule
\end{tabular}
\end{table}

\section{Baseline Fairness Checks (v29; Seed 1337)}
\label{app:baseline_fairness}

To address concerns that the full-rank baseline might be under-tuned under a short training horizon, we report (i) a small learning-rate sweep for the baseline at 6000 steps and (ii) a longer-horizon comparison at 12000 steps (seed 1337). Lower is better.

\begin{table}[htbp]
\centering
\small
\caption{Baseline LR sweep and longer-horizon comparison (FineWeb-Edu v29; seed 1337).}
\label{tab:baseline_fairness}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Run} & \textbf{Steps} & \textbf{Best Val Loss} \\
\midrule
Baseline ($3\times 10^{-4}$) & 6000 & 6.296 \\
Baseline ($2\times 10^{-4}$) & 6000 & 6.052 \\
Baseline ($4\times 10^{-4}$) & 6000 & 6.791 \\
Baseline ($3\times 10^{-4}$) & 12000 & 6.133 \\
Decoupled 48/96 ($3\times 10^{-4}$) & 12000 & 5.657 \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{plain}
\bibliography{references}

\end{document}
