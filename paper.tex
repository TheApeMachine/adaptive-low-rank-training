\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{microtype}  % Better text flow, reduces overfull hbox
\usepackage[margin=1in]{geometry}  % Consistent margins

% Custom colors
\definecolor{accent}{RGB}{45, 106, 79}

\title{\textbf{Decoupled Bottleneck Attention:} \\[0.3em] 
\large Scaling Efficient Transformers via Low-Rank Semantic Routing}

\author{
  Daniel Owen van Dommelen\\
  \textit{Independent Research}\\
  \texttt{theapemachine@gmail.com}
}

\date{December 2025}

\begin{document}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
The Key-Value cache in Transformer models scales linearly with sequence length and model dimension, creating a critical memory bottleneck for long-context inference. While techniques like Grouped-Query Attention (GQA) reduce cache size by sharing key-value heads, they preserve the full computational cost of attention scoring in high-dimensional space.

We propose \textbf{Decoupled Bottleneck Attention}, an architectural modification that exploits the empirical observation that \textit{semantic routing}---deciding which tokens attend to which---operates in a low-rank subspace ($r \approx 32$), while \textit{positional geometry} requires higher fidelity ($r \approx 64$). By decoupling these concerns into separate projection paths, we achieve:
\begin{itemize}
    \item \textbf{Validated end-to-end memory claim at 128k.} We provide a production benchmark (\path{production/bench_end_to_end_memory.py}) that measures device memory deltas from KV-cache allocation at 128k context and reports (i) architecture-only savings (standard FP16 $\rightarrow$ decoupled FP16), (ii) quantization-only savings (decoupled FP16 $\rightarrow$ hetero Q4/Q8/Q4), and (iii) end-to-end savings (standard FP16 $\rightarrow$ hetero Q4/Q8/Q4).
    \item \textbf{Evidence at two scales (FineWeb-Edu only).} We run a longer-horizon local suite on FineWeb-Edu 100M tokens for robust comparisons across baselines, and confirm the main trend at 1B model scale on an A100 using a much larger FineWeb token corpus (20B tokens) with a clean baseline-vs-decoupled comparison.
    \item \textbf{Robust alternative to Grouped-Query Attention (GQA).} On FineWeb-Edu, we compare against a GQA baseline and show that reducing the interaction rank is a competitive compression strategy; results are summarized in our tables generated from the manifest-run artifacts.
\end{itemize}

Across our FineWeb-Edu suites, the decoupled bottleneck matches or improves validation loss relative to strong baselines while enabling substantially smaller KV caches for long-context inference.

\end{abstract}

\paragraph{Keywords:}
Transformer, attention mechanism, low-rank, KV-cache, memory efficiency, quantization, long context, rotary position embeddings

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}

Modern Transformer architectures \cite{vaswani2017attention} achieve remarkable performance across language modeling, translation, and reasoning tasks. However, their quadratic attention complexity and linear KV-cache growth present fundamental scalability challenges for long-context applications.

\subsection{The Redundancy Hypothesis}

We begin with a simple observation: in a 512-dimensional layer, the neurons are not independent. They move in \textit{sympathetic clusters}---correlated groups that reduce the intrinsic dimensionality of the representation. Prior work on LoRA \cite{hu2021lora} demonstrated that weight \textit{updates} during fine-tuning are low-rank (typically $r \leq 64$). Recent work on gradient dynamics \cite{refael2024adarankgrad} shows that optimization naturally collapses to low rank. We extend this observation to argue that the \textit{architecture itself}---specifically the attention mechanism---should be structurally constrained to match this intrinsic rank.

Empirical measurements from our experiments show that the \textit{Q/K projection activations} feeding attention have low entropy effective rank (typically in the tens of dimensions, far below the nominal width; Appendix~\ref{app:effective_rank}). This aligns with theoretical analysis by Bhojanapalli et al. \cite{bhojanapalli2020lowrank}, who identified a ``low-rank bottleneck'' in multi-head attention, and recent work by Kobayashi et al. \cite{kobayashi2024weightdecay} showing that weight decay actively induces rank reduction during training. Wang et al. \cite{wang2025lowdim} further demonstrate that attention outputs are approximately 60\% low-dimensional, adding to low-dimensional residual subspaces. Crucially, Refael et al. \cite{refael2024adarankgrad} proved that gradient rank \textit{decreases monotonically} during training, asymptotically approaching rank one---providing theoretical justification for why architectural bottlenecks become increasingly appropriate as training progresses. Chiang \& Yogatama \cite{chiang2025rope} show that RoPE may cause dimension inefficiency for long-distance retrieval, supporting our use of higher dimensions (64) for the geometric path.

\subsection{Comparison with Existing Approaches}

\paragraph{Grouped-Query Attention (GQA).}
While Grouped-Query Attention \cite{ainslie2023gqa} successfully reduces KV-cache memory by sharing key-value heads across multiple query heads, it maintains the full computational cost of the query projection and attention scoring in the high-dimensional space. Each query still operates in $\mathbb{R}^{d}$, and every attention score still requires a $d$-dimensional dot product---GQA merely amortizes the \textit{storage} cost, not the \textit{interaction} cost.

Our Bottleneck approach reduces both memory \textit{and} compute by compressing the interaction manifold. Rather than sharing high-dimensional KV pairs, we project queries and keys into a low-rank semantic subspace ($r \ll d$) \textit{before} computing attention, reducing dot-product complexity from $O(n^2 d)$ to $O(n^2 r)$.

\paragraph{Multi-Head Latent Attention (MLA).}
DeepSeek-V2 \cite{deepseek2024v2} introduced MLA, which compresses KV storage into a latent vector, achieving 93\% cache reduction. However, MLA \textit{up-projects} during the forward pass to perform attention in the original high-dimensional space. Our method remains low-rank throughout, saving both memory and compute.

\paragraph{Disentangled Attention.}
DeBERTa \cite{he2020deberta} pioneered the separation of content and position representations in attention scoring. We adopt this disentanglement principle but leverage it for \textit{efficiency}: applying aggressive compression to the semantic (content) path while preserving fidelity in the geometric (position) path.

\subsection{Contributions}

\begin{enumerate}
    \item We demonstrate that attention routing can be performed in $\sim$32 dimensions without perplexity degradation, while positional encoding requires $\sim$64 dimensions for RoPE fidelity.
    \item We propose \textbf{Decoupled Bottleneck Attention}, which separates semantic and geometric scoring paths with asymmetric dimensionality.
    \item We introduce and \textbf{evaluate} a \textbf{Null Token} mechanism that provides an explicit ``attend nowhere'' option (beneficial in some low-rank regimes, but not universally; Appendix~\ref{app:decoupled_ablations}).
    \item We report measured KV-cache footprints using production tooling (see \path{production/bench_end_to_end_memory.py}) and quantify end-to-end memory deltas under heterogeneous KV-cache quantization at 128k context.
\end{enumerate}

% ============================================================================
% RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related_work}

\paragraph{Low-rank and approximate attention.}
A long line of work seeks to reduce the quadratic cost of attention by approximating the score computation or constraining its rank. Linformer \cite{wang2020linformer} projects keys and values into a lower-dimensional subspace along the sequence dimension, yielding linear-time attention under a low-rank assumption. Kernel and hashing methods such as Performer \cite{choromanski2021performer} and Reformer \cite{kitaev2020reformer} similarly reduce attention cost via randomized features or locality-sensitive hashing. Our setting is different: we train standard causal LMs, but explicitly reduce the query/key interaction dimension inside each layer, targeting both compute (\(O(n^2 r)\)) and KV-cache memory (\(O(n r)\)).

\paragraph{Sparse/local attention for long documents.}
Sparse patterns (e.g., sliding window with global tokens) as in Longformer \cite{beltagy2020longformer} and BigBird \cite{zaheer2020bigbird} reduce attention compute while retaining access to distant context. However, for autoregressive decoding these methods still accumulate a KV cache whose size grows linearly with context length. Our work instead reduces the per-token cache footprint, which is complementary to sparse attention and other long-context strategies \cite{huang2023longcontextsurvey}.

\paragraph{KV-cache optimization.}
Sharing KV heads reduces cache storage by amortizing keys and values across query heads (MQA/GQA) \cite{shazeer2019mqa,ainslie2023gqa}. Latent KV schemes such as MLA compress the cache into a lower-dimensional latent that is expanded during attention \cite{deepseek2024v2}. Orthogonally, quantizing the KV cache reduces memory at fixed architecture \cite{hooper2024kvquant,li2025commvq}. Our decoupled bottleneck reduces the interaction dimension before scoring (saving compute) and also makes heterogeneous KV quantization natural: semantic keys can often be quantized more aggressively than geometric keys.

\paragraph{Expressiveness limits and structured alternatives.}
Reducing interaction rank too far can harm representation power: theory and empirical evidence show regimes where increasing heads under fixed head dimension does not recover lost capacity \cite{bhojanapalli2020lowrank,amsel2025qualityheads}. Recent structured-matrix formulations aim to increase effective rank without full cost by parameterizing attention maps with richer structured operators \cite{kuang2025structuredmatrices}. Decoupling is a simple architectural compromise: we keep a higher-dimensional geometric path (with RoPE) while aggressively compressing only the semantic routing path.

% ============================================================================
% 2. METHODOLOGY
% ============================================================================
\section{Methodology}

\subsection{Standard Multi-Head Attention}

In standard scaled dot-product attention with $H$ heads:
\begin{equation}
    \text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\end{equation}
where $Q, K, V \in \mathbb{R}^{n \times d}$ are obtained by linear projection from the input $X \in \mathbb{R}^{n \times d_{\text{model}}}$:
\begin{equation}
    Q = XW_Q, \quad K = XW_K, \quad V = XW_V
\end{equation}
with $W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d}$. For language modeling with context length $n$ and dimension $d$, the KV-cache requires $O(2 \cdot L \cdot n \cdot d)$ memory, where $L$ is the number of layers.

\subsection{Bottleneck Attention}

We introduce a simple modification: project $Q$ and $K$ to a lower-dimensional space \textit{before} computing attention scores.\footnote{Our use of ``bottleneck'' refers to dimensionality reduction in the query/key space, distinct from Park et al.'s BAM \cite{park2018bam}, which applies channel and spatial attention in CNNs for computer vision.}
\begin{equation}
    Q' = XW_Q', \quad K' = XW_K'
\end{equation}
where $W_Q', W_K' \in \mathbb{R}^{d_{\text{model}} \times d_{\text{attn}}}$ with $d_{\text{attn}} \ll d_{\text{model}}$. The attention computation becomes:
\begin{equation}
    \text{Attn}_{\text{bottleneck}}(Q', K', V') = \text{softmax}\left(\frac{Q'K'^\top}{\sqrt{d_{\text{attn}}/H}}\right) V'
\end{equation}

This reduces the dot-product complexity from $O(n^2 \cdot d_{\text{model}})$ to $O(n^2 \cdot d_{\text{attn}})$ and the KV-cache from $O(n \cdot d_{\text{model}})$ to $O(n \cdot d_{\text{attn}})$.

\subsection{Decoupled Bottleneck Attention}

The key insight motivating decoupling is that \textit{semantic matching} (``is this token semantically related?'') and \textit{geometric positioning} (``how far away is this token?'') have different intrinsic dimensionality requirements.

We decompose the attention score into two additive components:
\begin{equation}
    \text{Score} = \underbrace{\frac{Q_{\text{sem}} K_{\text{sem}}^\top}{\sqrt{d_{\text{sem}}/H}}}_{\text{Semantic Path}} + \underbrace{\frac{Q_{\text{geo}} K_{\text{geo}}^\top}{\sqrt{d_{\text{geo}}/H}}}_{\text{Geometric Path}}
\end{equation}

where:
\begin{align}
    Q_{\text{sem}} &= XW_{Q,\text{sem}}, \quad K_{\text{sem}} = XW_{K,\text{sem}} \quad &(d_{\text{sem}} = 32) \\
    Q_{\text{geo}} &= XW_{Q,\text{geo}}, \quad K_{\text{geo}} = XW_{K,\text{geo}} \quad &(d_{\text{geo}} = 64)
\end{align}

Critically, we apply \textbf{Rotary Position Embeddings (RoPE)} \cite{su2021roformer} \textit{only} to the geometric path:
\begin{equation}
    Q_{\text{geo}}, K_{\text{geo}} \leftarrow \text{RoPE}(Q_{\text{geo}}, K_{\text{geo}}, \text{position})
\end{equation}

The semantic path operates on pure content similarity, while the geometric path encodes positional relationships. The value projection uses the combined dimension:
\begin{equation}
    V = XW_V, \quad W_V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{attn}}}
\end{equation}
where $d_{\text{attn}} = d_{\text{sem}} + d_{\text{geo}}$. In our production presets, $(d_{\text{sem}}, d_{\text{geo}})$ is chosen as a function of model size, with a higher-dimensional geometric path to preserve RoPE fidelity.

\subsection{The Null Token Mechanism}

Low-rank attention can become unstable when queries lack semantically appropriate keys. We introduce a learnable \textbf{null token} $k_\emptyset$ providing an explicit ``attend nowhere'' option:
\begin{equation}
    \text{Score}_{\text{null}} = \frac{Q_{\text{sem}} k_{\emptyset,\text{sem}}^\top}{\sqrt{d_{\text{sem}}/H}} + \frac{Q_{\text{geo}} k_{\emptyset,\text{geo}}^\top}{\sqrt{d_{\text{geo}}/H}}
\end{equation}

This score is concatenated to the attention matrix before softmax, allowing the model to ``dump'' attention mass when no key is appropriate, which can stabilize training at very low ranks. In our flagship decoupled preset, the null token is disabled by default and treated as an ablation (Appendix~\ref{app:decoupled_ablations}).

\subsection{Tied Q-K Projections}

For the semantic path, we optionally \textbf{tie} the query and key projections: $W_{Q,\text{sem}} = W_{K,\text{sem}}$. This enforces symmetric similarity (``A attends to B iff B attends to A''), which is appropriate for content matching but not for position-dependent relationships.

\subsection{Quantized Inference}

For inference, we apply aggressive quantization to the KV-cache. Recent work has demonstrated that 4-bit KV cache quantization preserves model quality remarkably well. Turboderp's ExLlamaV2 implementation \cite{turboderp2024qcache} showed Q4 cache performs comparably to FP16, and this capability has been integrated into production inference engines like llama.cpp \cite{llamacpp2024kvcache}. We implement block-wise Q4\_0 quantization following this approach:
\begin{equation}
    x_{\text{quantized}} = \text{round}\left(\frac{x}{\text{scale}}\right), \quad \text{scale} = \frac{\max(|x_{\text{block}}|)}{7}
\end{equation}
where each block of 32 elements shares a single FP16 scale factor. In the idealized limit (ignoring scale metadata) 4-bit values correspond to 0.5 bytes/value. With Q4\_0 block scales, the effective bytes/value is slightly larger (18 bytes per 32 values $\Rightarrow$ 0.5625 bytes/value), so the ideal 4$\times$ factor becomes $\approx 3.56\times$ in practice. Combined with dimension reduction, the per-layer KV-cache reduction is approximately:
\begin{equation}
    \text{Compression} \approx \underbrace{\frac{d_{\text{model}}}{d_{\text{attn}}}}_{\text{Dimension}} \times \underbrace{\frac{2~\text{bytes}}{0.5625~\text{bytes}}}_{\text{Q4\_0 (incl.\ scale)}} \;\;\approx\;\; \frac{d_{\text{model}}}{d_{\text{attn}}}\times 3.56.
\end{equation}
For a representative setting with $d_{\text{model}}/d_{\text{attn}} \approx 5.33$ (e.g., $768\!\rightarrow\!144$), homogeneous Q4\_0 KV-cache quantization implies an implementation-aligned compression of $\approx 5.33\times 3.56 \approx 19.0\times$ versus a standard FP16 baseline (before accounting for any heterogeneous policy choices).

\paragraph{Scaling arithmetic (context only; not validated at scale).}
The KV-cache memory at long context depends on the choice of attention dimension $d_{\text{attn}}$ at scale. For a rough Llama-like configuration (32 layers, $d_{\text{model}}=4096$, 128k context, batch=1), the FP16 KV cache is:
\[
M_{\text{FP16}} \approx 2 \cdot 32 \cdot 4096 \cdot 128\text{k} \cdot 2 \text{ bytes} \approx 64~\text{GiB}.
\]
With 4-bit KV-cache quantization (idealized 0.5 bytes/value), the memory becomes:
\[
M_{\text{Q4}} \approx 2 \cdot 32 \cdot d_{\text{attn}} \cdot 128\text{k} \cdot 0.5 \text{ bytes}.
\]
This yields two reference scenarios (for context):
\begin{itemize}
    \item \textbf{Constant-fraction $d_{\text{attn}}$ (e.g., $d_{\text{attn}}=768$):} $M_{\text{Q4}} \approx 3.0$~GiB, for an overall reduction of $\sim 21\times$.
    \item \textbf{Speculative fixed-rank $d_{\text{attn}}$ (intuition only):} If one could keep $d_{\text{attn}}$ roughly constant while scaling $d_{\text{model}}$ (e.g., $d_{\text{attn}}{=}96$ at $d_{\text{model}}{=}4096$), the same linear arithmetic yields an $\mathcal{O}(10^2)$ reduction versus a standard FP16 baseline.\footnote{This is the origin of the often-quoted ``168$\times$'': \((4096/96)\times 4 \approx 171\), sometimes rounded. Including Q4\_0 scale metadata gives \((4096/96)\times (2/0.5625)\approx 152\). We do \emph{not} validate fixed-rank scaling in this work.}
\end{itemize}
The architectural contribution is the \textit{dimension reduction} (the ratio $4096/d_{\text{attn}}$); the additional factor of $4\times$ comes from standard 16$\rightarrow$4-bit quantization (idealized). For fair comparisons, note that GQA caches can also be quantized; e.g., an $8\times$ GQA KV cache with Q4 would already yield $\sim$32$\times$ reduction vs FP16 standard. We therefore treat fixed-rank scaling numbers as back-of-the-envelope upper bounds, not a primary experimental claim.

\paragraph{Heterogeneous KV-cache quantization (decoupled).}
A practical benefit of decoupling is that it enables \textit{heterogeneous} KV-cache quantization: we can compress the semantic path more aggressively (e.g., Q4) while keeping the geometric (RoPE) path at higher fidelity (e.g., Q8). As a lightweight sanity check, we compare FP16 caches to a decoupled policy with $K_{\text{sem}}=\text{Q4}$, $K_{\text{geo}}=\text{Q8}$, $V=\text{Q4}$ and a 128-token FP16 residual window. On a small held-out calibration slice from FineWeb-Edu, this policy yields a negligible quality change while providing a large additional memory reduction.\footnote{We provide a production benchmark script (\path{production/bench_end_to_end_memory.py}) that directly measures allocated KV-cache bytes and device memory deltas at 128k context for this exact policy versus FP16, reporting architecture-only, quant-only, and end-to-end factors.}

While we report training throughput in our experiments, the theoretical FLOPs reduction in the attention mechanism ($O(n^2 d) \to O(n^2 r)$) implies a proportional speedup in the \textit{prefill phase} of inference, where the KV-cache is populated. For autoregressive decoding, the memory bandwidth savings from the smaller cache dominate latency improvements.

% ============================================================================
% 3. EXPERIMENTS
% ============================================================================
\section{Experiments}

\subsection{Experimental Setup}

\paragraph{Reproducibility discipline (production-only).}
All paper experiments are executed through a single implementation (\path{production/}) and a single CLI surface (\path{main.py}). Each run directory contains a \texttt{command.txt}, \texttt{resolved\_config.json}, and \texttt{resolved\_run.json} written before training starts. A canonical manifest (\path{paper\_manifest.json}) and one-command harness (\path{run\_paper\_manifest.py}) are used to eliminate flag ambiguity.

\paragraph{Datasets (FineWeb-Edu only).}
We exclusively use FineWeb-Edu tokenized with a GPT-2 compatible 50,257-token vocabulary. We run two dataset scales: a 100M-token suite for longer-horizon comparisons on local hardware, and a 20B-token suite for scale confirmation on an A100.

\paragraph{KV-cache memory measurement at 128k.}
For each run, we produce a \texttt{mem128k.json} artifact by executing \path{production/bench\_end\_to\_end\_memory.py} at 128k context. For decoupled runs we additionally report the decomposition (standard FP16 $\rightarrow$ decoupled FP16 $\rightarrow$ hetero Q4/Q8/Q4) used for the paper's end-to-end memory claim.

\subsection{FineWeb-Edu Results}

We summarize the main FineWeb-Edu results from production manifest runs in Table~\ref{tab:main}. The table is generated directly from run artifacts (logs + \texttt{mem128k.json}).

\input{assets/table_main.tex}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/fig_convergence.png}}
\caption{Validation loss curves on FineWeb-Edu (manifest-selected subset).}
\label{fig:convergence_fineweb}
\end{figure}

\subsection{Ablation Studies}

\paragraph{Wide Residual Stream Hypothesis.}
Comparing ``Small Model'' ($d_{\text{model}} = 128$) to ``Bottleneck 128'' ($d_{\text{model}} = 512$, $d_{\text{attn}} = 128$), we observe a 0.26 loss gap (5.74 vs 5.48) and severe overfitting in the small model. This confirms that the \textit{residual stream} must remain wide; only the \textit{attention interaction} can be compressed.

\paragraph{GQA vs. Bottleneck (Head Sharing vs. Interaction Rank).}
We compared against Grouped-Query Attention (GQA) on FineWeb-Edu. While GQA reduces KV storage by sharing key-value heads, it retains full-rank query projections and scoring in high-dimensional space. In contrast, the Bottleneck architecture reduces the interaction dimension directly (Table~\ref{tab:main}).

\paragraph{Decoupled vs. Bottleneck (Separating Content and Geometry).}
Holding the same total attention dimension fixed, the Decoupled Bottleneck can improve validation loss at essentially the same KV footprint, suggesting semantic/geometric separation improves optimization and generalization at this scale (Table~\ref{tab:main}).

\paragraph{Long Context Stability.}
Appendix~\ref{app:long_context} reports two lightweight probes: teacher-forced RoPE extrapolation (256/512/1024 contexts) and a passkey needle-in-a-haystack prompt. These probes are intended as regression tests (not downstream evaluations); we report likelihood-based needle effects because strict next-token retrieval accuracy is often 0\% for base LMs.

\subsection{Memory-Quality Trade-off}

Figure~\ref{fig:pareto} shows the quality--efficiency trade-off on FineWeb-Edu. KV-cache memory at 128k context is derived from the production memory benchmark (\texttt{mem128k.json}) and compared against each model's best validation loss.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{\detokenize{assets/fig_pareto_memory_vs_loss.png}}
\caption{Quality--efficiency comparison on FineWeb-Edu. Points show best validation loss versus KV-cache memory at 128k context (FP16).}
\label{fig:pareto}
\end{figure}

\subsection{Memory Footprint Analysis}

Table~\ref{tab:memory} gives an illustrative KV-cache scaling projection for a 128k context in a Llama-like configuration (32 layers, $d_{\text{model}} = 4096$). We intentionally \emph{do not} foreground optimistic fixed-rank ``upper bound'' numbers here; the experimentally grounded takeaway is the linear dependence on the interaction dimension and the fact that architectural reduction composes multiplicatively with KV-cache quantization. For paper numbers we rely on the measured allocation deltas reported by \path{production/bench_end_to_end_memory.py}.

\begin{table}[htbp]
\centering
\small
\caption{KV-Cache Memory for 128k Context (Llama-like scale; projected)}
\label{tab:memory}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Architecture} & \textbf{VRAM} & \textbf{Compression} \\
\midrule
Standard (FP16) & 64.0 GB & 1$\times$ \\
GQA (32Q/4KV; FP16) & 8.0 GB & 8$\times$ \\
GQA (32Q/4KV; Q4, ideal) & 2.0 GB & 32$\times$ \\
MLA (FP16) & 4.3 GB & 15$\times$ \\
Bottleneck (FP16) & 1.5 GB & 43$\times$ \\
Decoupled (Q4, constant-fraction $d_{\text{attn}}{=}768$) & 3.0 GB & 21$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{\detokenize{assets/memory_footprint.png}}
\caption{KV-cache memory comparison at 128k context (illustrative scaling projection). The experimentally grounded point is that KV-cache memory scales linearly with $d_{\text{attn}}$; combining this with standard KV-cache quantization yields multiplicative savings. Large fixed-rank upper-bound numbers are omitted from the main narrative and treated as speculative arithmetic (Section~2.6).}
\label{fig:memory}
\end{figure}

% ============================================================================
% 4. DISCUSSION
% ============================================================================
\section{Discussion}

\subsection{Why Does Low-Rank Attention Work?}

We hypothesize two complementary explanations:

\paragraph{Intrinsic Dimensionality.}
Following Aghajanyan et al. \cite{aghajanyan2021intrinsic}, natural language representations lie on low-dimensional manifolds. The attention mechanism's role is \textit{routing}---selecting which tokens to aggregate---not computing complex transformations. Routing decisions are inherently low-entropy and thus low-rank.

\paragraph{Regularization Effect.}
The bottleneck acts as an implicit regularizer, preventing the model from memorizing spurious token-pair correlations. In our FineWeb-Edu runs, reducing the interaction rank can improve generalization relative to the full-rank baseline.

\paragraph{Gradient Rank Dynamics.}
AdaRankGrad \cite{refael2024adarankgrad} proves that gradient rank decreases monotonically during training, eventually approaching rank one. This suggests that \textit{architectural} bottlenecks become increasingly appropriate as training progresses---the model naturally ``wants'' to operate in a low-rank subspace. By hard-wiring this constraint from the start, we may accelerate convergence by matching the architecture to the optimization landscape.

\subsection{When to Use Each Architecture}

Our experiments are organized into a local suite (FineWeb-Edu 100M) for broader comparisons and a scale suite (FineWeb-Edu 20B tokens) for confirmation.

\begin{itemize}
    \item \textbf{Decoupled Bottleneck:} On FineWeb-Edu, the decoupled bottleneck is a strong default that preserves the KV memory benefits of low-rank attention while enabling \textbf{heterogeneous quantization} (e.g., Q4 semantic, Q8 geometric).
    \item \textbf{Standard Attention:} A strong baseline and simplest implementation, but can be memory-inefficient for long contexts.
\end{itemize}

\paragraph{Recommendation.}
For \textit{training}, iterate on the local FineWeb-Edu suite and validate at scale with the A100 suite. For \textit{inference} under memory constraints, use Decoupled with heterogeneous quantization (aggressively compress semantic, preserve geometric fidelity).

\paragraph{Flash/SDPA compatibility.}
Decoupled Bottleneck Attention can be implemented using PyTorch's fused \texttt{scaled\_dot\_product\_attention} by concatenating the scaled semantic and geometric Q/K projections along the head dimension, making it compatible with modern Flash Attention kernels.

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Sensitivity to setup:} Results can depend on the training recipe and implementation details. This motivates replication across seeds, longer training, and additional datasets.
    \item Experiments are limited to $\sim$140M parameter models (up to $d_{\text{model}}{=}768$) trained on a 100M-token dataset. Verification at 7B+ scale is needed.
    \item The optimal $(d_{\text{sem}}, d_{\text{geo}})$ split may vary with model scale.
    \item We have not evaluated on downstream tasks (e.g., MMLU, HellaSwag).
    \item Throughput measurements are from training; inference latency benchmarks are future work.
\end{itemize}

% ============================================================================
% 5. CONCLUSION
% ============================================================================
\section{Conclusion}

We have demonstrated that attention in Transformers contains significant redundancy. On FineWeb-Edu, the Decoupled Bottleneck architecture can match or surpass strong baselines while enabling substantial KV-cache savings at long context (Table~\ref{tab:main}).

The core insight is architectural: \textbf{Attention is a router, not a processor.} The heavy computation should happen in the feedforward layers (which we leave at full rank), while attention merely selects which tokens to aggregate. By matching the architecture to this functional role, we unlock dramatic efficiency gains.

Our Decoupled Bottleneck Attention separates semantic matching from positional geometry, allowing aggressive compression on the former while preserving RoPE fidelity on the latter. Combined with 4-bit KV-cache quantization, the memory arithmetic suggests that 128k-context inference can become \textit{feasible} on consumer hardware under fixed-rank scaling assumptions (Figure~\ref{fig:memory}); however, this is a projection and we do not claim validated 128k \textit{quality} in this work.

\paragraph{Future Work.}
We plan to: (1) validate at 7B+ scale where the efficiency gains compound; (2) explore learned mixing weights between semantic and geometric paths; (3) test robustness across seeds, longer training, and additional datasets to understand when decoupling improves optimization; and (4) benchmark inference latency on production hardware. In parallel, we are migrating from the monolithic research prototype to a modular production-oriented implementation (see the repository's \path{production/} code), with an evolving self-optimization layer for KV-cache policies and resource-aware inference; future work will validate functional parity and quantify real-world latency/memory trade-offs.

% ============================================================================
% STATEMENTS
% ============================================================================
\section*{Statements and Declarations}

\paragraph{Conflict of Interest.}
The author declares no competing interests. This research was conducted independently without corporate affiliation or funding from entities with financial interests in the outcomes.

\paragraph{Data Availability.}
All datasets used in this study are publicly available: FineWeb-Edu is available from Hugging Face. The code, trained model checkpoints, and all experimental logs are available at \url{https://github.com/theapemachine/experiments}.

\paragraph{Funding.}
This research was conducted without external funding. All computational resources were provided by the author.

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Replication Plots for FineWeb-Edu (legacy)}
\label{app:v29_seed1338}

This section contains legacy replication plots from earlier FineWeb-Edu runs. In the FineWeb-only reboot, canonical paper numbers are generated from \path{paper\_manifest.json} and \path{generate\_paper\_figures.py}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1338_convergence.png}}
\caption{Validation loss curves on FineWeb-Edu (100M-token dataset; seed 1338).}
\label{fig:convergence_fineweb_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1338_early_convergence.png}}
\caption{Early convergence view on FineWeb-Edu (seed 1338), highlighting the first phase of optimization.}
\label{fig:early_convergence_fineweb_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1338_comparison_bar.png}}
\caption{Final validation loss comparison on FineWeb-Edu (v29; seed 1338). Lower is better.}
\label{fig:comparison_bar_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1338_pareto.png}}
\caption{Quality--efficiency comparison on FineWeb-Edu (v29; seed 1338).}
\label{fig:pareto_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{\detokenize{assets/m4max_seed1338_kv_memory_128k.png}}
\caption{Measured KV-cache memory at 128k context (v29; seed 1338).}
\label{fig:kv_memory_128k_seed1338}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1339_convergence.png}}
\caption{Validation loss curves on FineWeb-Edu (100M-token dataset; seed 1339).}
\label{fig:convergence_fineweb_seed1339}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_seed1339_early_convergence.png}}
\caption{Early convergence view on FineWeb-Edu (seed 1339), highlighting the first phase of optimization.}
\label{fig:early_convergence_fineweb_seed1339}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1339_comparison_bar.png}}
\caption{Final validation loss comparison on FineWeb-Edu (v29; seed 1339). Lower is better.}
\label{fig:comparison_bar_seed1339}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{\detokenize{assets/m4max_seed1339_pareto.png}}
\caption{Quality--efficiency comparison on FineWeb-Edu (v29; seed 1339).}
\label{fig:pareto_seed1339}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{\detokenize{assets/m4max_seed1339_kv_memory_128k.png}}
\caption{Measured KV-cache memory at 128k context (v29; seed 1339).}
\label{fig:kv_memory_128k_seed1339}
\end{figure}

\section{Effective Rank Evidence (Q/K Projection Activations)}
\label{app:effective_rank}

We compute singular value spectra and entropy-based effective rank for the \textit{Q/K projection activations} (post-linear outputs feeding attention) in trained checkpoints, by capturing projection outputs on a fixed token slice and computing singular values of their empirical covariance. We use the standard entropy effective rank:
\[
\mathrm{eRank} = \exp(H(p)), \quad p_i=\frac{\sigma_i}{\sum_j \sigma_j},
\]
where $\{\sigma_i\}$ are singular values and $H(\cdot)$ is Shannon entropy. The script to reproduce this figure is included in the repository; the figure below is generated automatically when that script is run.

\begin{figure}[htbp]
\centering
\IfFileExists{assets/m4max_rank_evidence.png}{
  \includegraphics[width=0.95\textwidth]{\detokenize{assets/m4max_rank_evidence.png}}
}{
  \fbox{\parbox{0.92\textwidth}{\textbf{Missing figure:} \path{assets/m4max_rank_evidence.png}.\\
  Run the rank analysis script to generate this figure.}}
}
\caption{Singular value spectra and entropy-based effective rank for Q/K projection activations across layers (example: v29 checkpoints).}
\label{fig:rank_evidence}
\end{figure}

\section{Long-Context Sanity Checks}
\label{app:long_context}

To complement the theoretical KV-cache scaling analysis, we include two lightweight long-context probes: (1) teacher-forced loss at increasing context lengths (RoPE extrapolation), and (2) a passkey ``needle-in-a-haystack'' next-token retrieval probe. These are not full downstream evaluations, but they help catch obvious long-context regressions and RoPE failures.

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_baseline_rope_extrapolation.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_baseline_rope_extrapolation.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_baseline_rope_extrapolation.png}}}
}
\caption{Baseline}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_decoupled_rope_extrapolation.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_decoupled_rope_extrapolation.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_decoupled_rope_extrapolation.png}}}
}
\caption{Decoupled}
\end{subfigure}
\caption{RoPE extrapolation probe (teacher-forced). Figures are generated by \texttt{test\_rope\_extrapolation\_v29.py}.}
\label{fig:rope_extrapolation_appendix}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_baseline_needle_haystack.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_baseline_needle_haystack.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_baseline_needle_haystack.png}}}
}
\caption{Baseline}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_decoupled_needle_haystack.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_decoupled_needle_haystack.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_decoupled_needle_haystack.png}}}
}
\caption{Decoupled}
\end{subfigure}
\vspace{0.75em}

\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_baseline_needle_haystack_delta_nll.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_baseline_needle_haystack_delta_nll.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_baseline_needle_haystack_delta_nll.png}}}
}
\caption{Baseline: $\Delta$NLL (with needle $-$ no needle)}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_decoupled_needle_haystack_delta_nll.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_decoupled_needle_haystack_delta_nll.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_decoupled_needle_haystack_delta_nll.png}}}
}
\caption{Decoupled: $\Delta$NLL (with needle $-$ no needle)}
\end{subfigure}

\vspace{0.75em}

\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_baseline_needle_haystack_log10_p_ratio.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_baseline_needle_haystack_log10_p_ratio.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_baseline_needle_haystack_log10_p_ratio.png}}}
}
\caption{Baseline: $\log_{10}\!\left(p_{\text{with}}/p_{\text{without}}\right)$}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\centering
\IfFileExists{assets/m4max_decoupled_needle_haystack_log10_p_ratio.png}{
  \includegraphics[width=\textwidth]{\detokenize{assets/m4max_decoupled_needle_haystack_log10_p_ratio.png}}
}{
  \fbox{\parbox{0.95\textwidth}{\textbf{Missing:} \path{assets/m4max_decoupled_needle_haystack_log10_p_ratio.png}}}
}
\caption{Decoupled: $\log_{10}\!\left(p_{\text{with}}/p_{\text{without}}\right)$}
\end{subfigure}

\caption{Passkey needle-in-a-haystack probe. Top row: strict next-token accuracy (often all-zero for base LMs). Middle/bottom: likelihood-based needle effect metrics which remain informative even when strict accuracy is zero. Figures are generated by \texttt{test\_needle\_haystack\_v29.py}.}
\label{fig:needle_haystack_appendix}
\end{figure}

\section{Decoupled Ablations (v29; Seed 1337)}
\label{app:decoupled_ablations}

To support claims about optional stabilizers (null token, tied Q--K projections) and positional encoding, we report a small ablation suite for Decoupled 48/96 on FineWeb-Edu (6000 steps; seed 1337). Lower is better.

\begin{table}[htbp]
\centering
\small
\caption{Decoupled 48/96 ablations (FineWeb-Edu v29; seed 1337; 6000 steps).}
\label{tab:decoupled_ablations}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Variant} & \textbf{Best Val Loss} & \textbf{$\Delta$ vs base} \\
\midrule
Decoupled 48/96 (base) & 5.879 & +0.000 \\
$-$ null token (\texttt{no\_null}) & 5.843 & $-0.035$ \\
$+$ tied Q--K (\texttt{tie\_qk}) & 5.926 & $+0.047$ \\
$+$ null token (\texttt{null}) & 6.395 & $+0.516$ \\
$-$ RoPE (\texttt{no\_rope}) & 6.446 & $+0.567$ \\
\bottomrule
\end{tabular}
\end{table}

\section{Baseline Fairness Checks (v29; Seed 1337)}
\label{app:baseline_fairness}

To address concerns that the full-rank baseline might be under-tuned under a short training horizon, we report (i) a small learning-rate sweep for the baseline at 6000 steps and (ii) a longer-horizon comparison at 12000 steps (seed 1337). Lower is better.

\begin{table}[htbp]
\centering
\small
\caption{Baseline LR sweep and longer-horizon comparison (FineWeb-Edu v29; seed 1337).}
\label{tab:baseline_fairness}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Run} & \textbf{Steps} & \textbf{Best Val Loss} \\
\midrule
Baseline ($3\times 10^{-4}$) & 6000 & 6.296 \\
Baseline ($2\times 10^{-4}$) & 6000 & 6.052 \\
Baseline ($4\times 10^{-4}$) & 6000 & 6.791 \\
Baseline ($3\times 10^{-4}$) & 12000 & 6.133 \\
Decoupled 48/96 ($3\times 10^{-4}$) & 12000 & 5.657 \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{plain}
\bibliography{references}

\end{document}
